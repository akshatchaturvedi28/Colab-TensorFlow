{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_Text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMwwqjeMb4c4l/V2JvZH53p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshatchaturvedi28/Collab-TensorFlow/blob/master/TF_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z5vmLKIcuMt",
        "colab_type": "text"
      },
      "source": [
        "**TF TEXT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T10RG0ccwGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G54-sNqdFGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j48dgP8ceBx1",
        "colab_type": "text"
      },
      "source": [
        "**TOKENIZATION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBhDGLdjeFTh",
        "colab_type": "text"
      },
      "source": [
        "WHITE SPACE TOKENNIZER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6oNWYRMd9uX",
        "colab_type": "code",
        "outputId": "60d508b3-9137-4e2a-a444-bc43da2b9913",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer = text.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize(['Everything, not saved is Lost.'])\n",
        "print(tokens.to_list())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[b'Everything,', b'not', b'saved', b'is', b'Lost.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83BDM6xVfS3K",
        "colab_type": "text"
      },
      "source": [
        "UNICODE SCRIPT TOKENIZER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8IiDpD6fY9U",
        "colab_type": "code",
        "outputId": "e07dc1f9-78ef-47b4-eb97-f635f6abdfb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer = text.UnicodeScriptTokenizer()\n",
        "tokens = tokenizer.tokenize(['Everything, not saved is Lost.'])\n",
        "print(tokens.to_list())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[b'Everything', b',', b'not', b'saved', b'is', b'Lost', b'.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih4sHitTgLvg",
        "colab_type": "text"
      },
      "source": [
        "OFFSETS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoTGEBYwfyYK",
        "colab_type": "code",
        "outputId": "5f473841-e25d-4904-e774-097b5b68135d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "tokenizer = text.UnicodeScriptTokenizer()\n",
        "(tokens, offset_start, offset_end) = tokenizer.tokenize_with_offsets(['Everything, not saved is lost'])\n",
        "print(tokens.to_list())\n",
        "print(offset_start.to_list())\n",
        "print(offset_end.to_list())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[b'Everything', b',', b'not', b'saved', b'is', b'lost']]\n",
            "[[0, 10, 12, 16, 22, 25]]\n",
            "[[10, 11, 15, 21, 24, 29]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSmxJPSlhBM0",
        "colab_type": "text"
      },
      "source": [
        "**Word Shape**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45Br5NFogvFc",
        "colab_type": "code",
        "outputId": "6c302d20-f4be-46c2-f5f0-dd230b1796f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "tokenizer = text.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize(['Everything, not saved is Lost.'])\n",
        "\n",
        "f1 = text.wordshape(tokens, text.WordShape.HAS_TITLE_CASE)\n",
        "\n",
        "f2 = text.wordshape(tokens, text.WordShape.IS_LOWERCASE)\n",
        "\n",
        "f3 = text.wordshape(tokens, text.WordShape.IS_UPPERCASE)\n",
        "\n",
        "f4 = text.wordshape(tokens, text.WordShape.HAS_NON_LETTER)\n",
        "\n",
        "print(f1.to_list())\n",
        "print(f2.to_list())\n",
        "print(f3.to_list())\n",
        "print(f4.to_list())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[True, False, False, False, True]]\n",
            "[[False, True, True, True, False]]\n",
            "[[False, False, False, False, False]]\n",
            "[[True, False, False, False, True]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B8_HUhViOyh",
        "colab_type": "text"
      },
      "source": [
        "**N GRAMS & SLIDING WINDOW**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw1o5TkNiCsk",
        "colab_type": "code",
        "outputId": "e0ae9df2-36f6-415b-b481-6d963b6fda00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer = text.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize(['Everything, not saved is Lost.'])\n",
        "\n",
        "#NGrams (n = 2)\n",
        "bigrams = text.ngrams(tokens, 2, reduction_type=text.Reduction.STRING_JOIN)\n",
        "\n",
        "print(bigrams.to_list())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[b'Everything, not', b'not saved', b'saved is', b'is Lost.']]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}