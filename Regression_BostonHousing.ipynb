{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regression_BostonHousing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPRTi9W8mpoTwuGYtuFhmKm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshatchaturvedi28/Collab-TensorFlow/blob/master/Regression_BostonHousing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7bKaDuhovY2",
        "colab_type": "code",
        "outputId": "4b2636fc-5847-46f6-aaf6-f5a1c5949bb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        }
      },
      "source": [
        "!pip install tensorflow==2.0.0-rc0\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_boston\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "tf.random.set_seed(7)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0-rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/4b/77f0965ec7e8a76d3dcd6a22ca8bbd2b934cd92c4ded43fef6bea5ff3258/tensorflow-2.0.0rc0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 47kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.34.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.1.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.17.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.12.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (3.1.0)\n",
            "Collecting tb-nightly<1.15.0a20190807,>=1.15.0a20190806\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/88/24b5fb7280e74c7cf65bde47c171547fd02afb3840cff41bcbe9270650f5/tb_nightly-1.15.0a20190806-py3-none-any.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 46.0MB/s \n",
            "\u001b[?25hCollecting tf-estimator-nightly<1.14.0.dev2019080602,>=1.14.0.dev2019080601\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/28/f2a27a62943d5f041e4a6fd404b2d21cb7c59b2242a4e73b03d9ba166552/tf_estimator_nightly-1.14.0.dev2019080601-py2.py3-none-any.whl (501kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 51.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (3.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.27.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0-rc0) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (1.0.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (45.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (3.2.1)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, tensorflow\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed tb-nightly-1.15.0a20190806 tensorflow-2.0.0rc0 tf-estimator-nightly-1.14.0.dev2019080601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YneXJb4FxIBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd7OLv3ao7NO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boston_dataset = load_boston()\n",
        "\n",
        "data_X = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
        "data_Y = pd.DataFrame(boston_dataset.target, columns=[\"target\"])\n",
        "data = pd.concat([data_X, data_Y], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZkP9cOVpM3i",
        "colab_type": "code",
        "outputId": "83095659-c92a-4cff-d224-f34d86568356",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "train, test = train_test_split(data, test_size=0.25, random_state=7)\n",
        "train, val = train_test_split(train, test_size=0.25, random_state=7)\n",
        "print(len(train), \"train examples\")\n",
        "print(len(val), \"validation examples\")\n",
        "print(len(test), \"test examples\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "284 train examples\n",
            "95 validation examples\n",
            "127 test examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIJRTv_zpTE4",
        "colab_type": "text"
      },
      "source": [
        "Converting the Pandas DataFrames into Tensorflow Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6nclDBApPmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('target')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKHHcvU5pVP6",
        "colab_type": "code",
        "outputId": "a8cf130c-9c12-426e-8a43-92c4909744c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "batch_size = 32\n",
        "train_ds = df_to_dataset(train, True, batch_size)\n",
        "val_ds = df_to_dataset(val, False, batch_size)\n",
        "test_ds = df_to_dataset(test, False, batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a48eh0dpalh",
        "colab_type": "text"
      },
      "source": [
        "**Defining Feature Columns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI1LTxOBq5IF",
        "colab_type": "code",
        "outputId": "94031886-1212-4ead-de54-9a2702eb1090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS    NOX  ...    TAX  PTRATIO       B  LSTAT  target\n",
              "0  0.00632  18.0   2.31   0.0  0.538  ...  296.0     15.3  396.90   4.98    24.0\n",
              "1  0.02731   0.0   7.07   0.0  0.469  ...  242.0     17.8  396.90   9.14    21.6\n",
              "2  0.02729   0.0   7.07   0.0  0.469  ...  242.0     17.8  392.83   4.03    34.7\n",
              "3  0.03237   0.0   2.18   0.0  0.458  ...  222.0     18.7  394.63   2.94    33.4\n",
              "4  0.06905   0.0   2.18   0.0  0.458  ...  222.0     18.7  396.90   5.33    36.2\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d2b24bec-d073-40d3-d6c6-3407b7ed605f",
        "id": "coZnmXaJ0Pg7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "# define feature_columns as a list of features using functions from tf.feature_column\n",
        "\n",
        "\n",
        "feature_columns = [tf.feature_column.numeric_column('CRIM'),\n",
        "tf.feature_column.numeric_column('ZN'),\n",
        "tf.feature_column.numeric_column('INDUS'),\n",
        "tf.feature_column.numeric_column('CHAS'),\n",
        "tf.feature_column.numeric_column('NOX'),\n",
        "tf.feature_column.numeric_column('RM'),\n",
        "tf.feature_column.numeric_column('AGE'),\n",
        "tf.feature_column.numeric_column('DIS'),\n",
        "tf.feature_column.bucketized_column(tf.feature_column.numeric_column('RAD'), boundaries=[2, 5]),\n",
        "tf.feature_column.numeric_column('TAX'),\n",
        "tf.feature_column.numeric_column('PTRATIO'),\n",
        "tf.feature_column.numeric_column('B'),\n",
        "tf.feature_column.numeric_column('LSTAT')]\n",
        "feature_columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[NumericColumn(key='CRIM', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='ZN', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='INDUS', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='CHAS', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='NOX', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='RM', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='AGE', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='DIS', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " BucketizedColumn(source_column=NumericColumn(key='RAD', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(2, 5)),\n",
              " NumericColumn(key='TAX', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='PTRATIO', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='B', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='LSTAT', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdszOTuLphyi",
        "colab_type": "text"
      },
      "source": [
        "**Building the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izuAwsihpfZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjGEaP0Awpm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([\n",
        "    feature_layer,\n",
        "    keras.layers.Dense(1, activation=None)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu7JxaKnpmwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=tf.keras.losses.MAPE , metrics=['mse'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MlQdTBAwmGd",
        "colab_type": "code",
        "outputId": "531efb57-23c2-4ddc-fdf1-984e65997a5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(train_ds, validation_data=val_ds, epochs=200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 21.3807 - mse: 52.5584 - val_loss: 0.0000e+00 - val_mse: 0.0000e+00\n",
            "Epoch 2/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 21.4997 - mse: 50.5495 - val_loss: 21.4901 - val_mse: 48.3737\n",
            "Epoch 3/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 21.4298 - mse: 52.7247 - val_loss: 21.7170 - val_mse: 50.1429\n",
            "Epoch 4/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 21.4174 - mse: 51.6521 - val_loss: 21.3340 - val_mse: 48.3871\n",
            "Epoch 5/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 21.2133 - mse: 52.1970 - val_loss: 21.6569 - val_mse: 50.8281\n",
            "Epoch 6/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 21.3735 - mse: 53.2310 - val_loss: 21.4587 - val_mse: 49.4973\n",
            "Epoch 7/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 21.5435 - mse: 52.4324 - val_loss: 21.3742 - val_mse: 48.8467\n",
            "Epoch 8/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 20.8613 - mse: 50.6485 - val_loss: 21.3241 - val_mse: 48.4721\n",
            "Epoch 9/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 21.0783 - mse: 55.5303 - val_loss: 21.6570 - val_mse: 50.8944\n",
            "Epoch 10/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 21.1974 - mse: 50.5306 - val_loss: 21.0864 - val_mse: 45.3100\n",
            "Epoch 11/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 21.7045 - mse: 51.6533 - val_loss: 21.9177 - val_mse: 53.6331\n",
            "Epoch 12/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 20.9384 - mse: 53.1000 - val_loss: 20.9792 - val_mse: 46.8885\n",
            "Epoch 13/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 21.6137 - mse: 51.3846 - val_loss: 21.5348 - val_mse: 51.4991\n",
            "Epoch 14/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 20.7724 - mse: 52.5868 - val_loss: 20.9588 - val_mse: 47.2255\n",
            "Epoch 15/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 20.8264 - mse: 51.8741 - val_loss: 21.3299 - val_mse: 49.9044\n",
            "Epoch 16/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 21.3964 - mse: 52.1073 - val_loss: 21.3181 - val_mse: 49.3597\n",
            "Epoch 17/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 20.8667 - mse: 52.2468 - val_loss: 20.9789 - val_mse: 46.6511\n",
            "Epoch 18/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 20.6020 - mse: 50.5027 - val_loss: 20.9542 - val_mse: 47.3185\n",
            "Epoch 19/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 21.3343 - mse: 52.2168 - val_loss: 21.3874 - val_mse: 50.8157\n",
            "Epoch 20/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 21.5294 - mse: 51.1133 - val_loss: 20.8164 - val_mse: 46.7095\n",
            "Epoch 21/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 20.2053 - mse: 52.2111 - val_loss: 21.1878 - val_mse: 49.7062\n",
            "Epoch 22/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 20.6783 - mse: 51.8356 - val_loss: 20.7747 - val_mse: 46.1481\n",
            "Epoch 23/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 20.6734 - mse: 51.8296 - val_loss: 21.1040 - val_mse: 49.6697\n",
            "Epoch 24/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 20.3595 - mse: 52.4366 - val_loss: 20.7037 - val_mse: 46.8641\n",
            "Epoch 25/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 20.4955 - mse: 51.1429 - val_loss: 20.9628 - val_mse: 49.6960\n",
            "Epoch 26/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 20.7091 - mse: 51.2985 - val_loss: 20.6993 - val_mse: 47.4074\n",
            "Epoch 27/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 20.4952 - mse: 52.1436 - val_loss: 20.7113 - val_mse: 47.5831\n",
            "Epoch 28/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 20.2285 - mse: 50.9450 - val_loss: 20.9485 - val_mse: 49.4083\n",
            "Epoch 29/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 20.3633 - mse: 51.9304 - val_loss: 20.6710 - val_mse: 47.5062\n",
            "Epoch 30/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 20.5350 - mse: 53.7279 - val_loss: 20.7457 - val_mse: 47.7003\n",
            "Epoch 31/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.9366 - mse: 50.0663 - val_loss: 20.4890 - val_mse: 46.6961\n",
            "Epoch 32/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.8924 - mse: 52.5568 - val_loss: 20.8672 - val_mse: 49.5414\n",
            "Epoch 33/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 20.7163 - mse: 53.2784 - val_loss: 20.6529 - val_mse: 47.7131\n",
            "Epoch 34/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.6230 - mse: 50.4706 - val_loss: 20.5358 - val_mse: 46.7383\n",
            "Epoch 35/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 20.4995 - mse: 50.5481 - val_loss: 20.8859 - val_mse: 50.1011\n",
            "Epoch 36/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 20.1205 - mse: 53.3245 - val_loss: 20.6759 - val_mse: 48.0918\n",
            "Epoch 37/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 20.1604 - mse: 49.8870 - val_loss: 20.4261 - val_mse: 46.6614\n",
            "Epoch 38/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 20.8189 - mse: 53.2988 - val_loss: 20.6856 - val_mse: 48.7410\n",
            "Epoch 39/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.5718 - mse: 50.7610 - val_loss: 20.3076 - val_mse: 46.3241\n",
            "Epoch 40/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.8055 - mse: 53.5222 - val_loss: 20.7082 - val_mse: 49.8080\n",
            "Epoch 41/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 20.5420 - mse: 51.6631 - val_loss: 20.5194 - val_mse: 48.5910\n",
            "Epoch 42/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 20.6083 - mse: 52.9706 - val_loss: 20.5010 - val_mse: 48.2141\n",
            "Epoch 43/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 20.4253 - mse: 52.0984 - val_loss: 20.5587 - val_mse: 48.1019\n",
            "Epoch 44/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.9967 - mse: 51.5821 - val_loss: 20.3755 - val_mse: 46.8127\n",
            "Epoch 45/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 20.1128 - mse: 49.8058 - val_loss: 20.5536 - val_mse: 47.8783\n",
            "Epoch 46/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 20.0556 - mse: 51.8337 - val_loss: 20.5927 - val_mse: 48.8136\n",
            "Epoch 47/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 20.5277 - mse: 53.1613 - val_loss: 20.1315 - val_mse: 44.7101\n",
            "Epoch 48/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 20.5233 - mse: 49.0603 - val_loss: 20.8172 - val_mse: 50.5458\n",
            "Epoch 49/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 20.2858 - mse: 54.1166 - val_loss: 20.1632 - val_mse: 46.9095\n",
            "Epoch 50/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 20.9214 - mse: 48.8660 - val_loss: 20.3830 - val_mse: 48.4891\n",
            "Epoch 51/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 20.1583 - mse: 54.3925 - val_loss: 20.6313 - val_mse: 50.2076\n",
            "Epoch 52/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.8991 - mse: 50.6411 - val_loss: 20.0377 - val_mse: 46.3601\n",
            "Epoch 53/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.7321 - mse: 52.3899 - val_loss: 20.2261 - val_mse: 47.9751\n",
            "Epoch 54/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 20.5755 - mse: 49.8029 - val_loss: 20.1042 - val_mse: 47.4480\n",
            "Epoch 55/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 20.1071 - mse: 54.6119 - val_loss: 20.8585 - val_mse: 51.4773\n",
            "Epoch 56/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.8705 - mse: 50.4273 - val_loss: 19.9106 - val_mse: 44.4567\n",
            "Epoch 57/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.7863 - mse: 48.8547 - val_loss: 20.7959 - val_mse: 50.9092\n",
            "Epoch 58/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.8689 - mse: 55.4326 - val_loss: 20.2074 - val_mse: 47.4624\n",
            "Epoch 59/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.7350 - mse: 48.7325 - val_loss: 19.9597 - val_mse: 46.2184\n",
            "Epoch 60/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.4081 - mse: 51.9868 - val_loss: 20.2117 - val_mse: 48.1951\n",
            "Epoch 61/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.9297 - mse: 53.4667 - val_loss: 20.4128 - val_mse: 49.2001\n",
            "Epoch 62/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.6095 - mse: 50.9450 - val_loss: 20.0043 - val_mse: 46.3890\n",
            "Epoch 63/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.6756 - mse: 50.2825 - val_loss: 20.3887 - val_mse: 48.8095\n",
            "Epoch 64/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.9338 - mse: 51.5193 - val_loss: 20.2068 - val_mse: 48.4600\n",
            "Epoch 65/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.6099 - mse: 51.7840 - val_loss: 20.0978 - val_mse: 47.8520\n",
            "Epoch 66/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 20.0601 - mse: 50.9545 - val_loss: 20.2335 - val_mse: 48.6995\n",
            "Epoch 67/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 20.0202 - mse: 50.7245 - val_loss: 19.7758 - val_mse: 46.0935\n",
            "Epoch 68/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.6478 - mse: 51.3211 - val_loss: 20.7932 - val_mse: 51.9156\n",
            "Epoch 69/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.4346 - mse: 52.5655 - val_loss: 19.7371 - val_mse: 45.5635\n",
            "Epoch 70/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.6731 - mse: 49.4120 - val_loss: 19.8429 - val_mse: 46.8676\n",
            "Epoch 71/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.6772 - mse: 52.2257 - val_loss: 20.5427 - val_mse: 50.8328\n",
            "Epoch 72/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.8365 - mse: 52.9279 - val_loss: 19.6651 - val_mse: 45.6355\n",
            "Epoch 73/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.6593 - mse: 50.0440 - val_loss: 19.7865 - val_mse: 46.7208\n",
            "Epoch 74/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.2330 - mse: 51.4391 - val_loss: 19.7580 - val_mse: 46.5954\n",
            "Epoch 75/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.9566 - mse: 47.9840 - val_loss: 19.6557 - val_mse: 46.3730\n",
            "Epoch 76/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 20.0013 - mse: 57.1864 - val_loss: 21.1689 - val_mse: 55.1159\n",
            "Epoch 77/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.8151 - mse: 51.1484 - val_loss: 19.5542 - val_mse: 44.4301\n",
            "Epoch 78/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.3664 - mse: 49.6809 - val_loss: 20.5990 - val_mse: 51.7191\n",
            "Epoch 79/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.2790 - mse: 52.9536 - val_loss: 19.5685 - val_mse: 45.5109\n",
            "Epoch 80/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.1013 - mse: 49.9536 - val_loss: 19.9520 - val_mse: 48.6809\n",
            "Epoch 81/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.6290 - mse: 51.3257 - val_loss: 19.7320 - val_mse: 47.8923\n",
            "Epoch 82/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 18.4152 - mse: 51.2406 - val_loss: 19.6591 - val_mse: 47.3331\n",
            "Epoch 83/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.6518 - mse: 50.7418 - val_loss: 20.0129 - val_mse: 49.3688\n",
            "Epoch 84/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 18.9784 - mse: 52.2782 - val_loss: 19.5217 - val_mse: 45.8576\n",
            "Epoch 85/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.2053 - mse: 50.4979 - val_loss: 20.1704 - val_mse: 49.9073\n",
            "Epoch 86/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.4054 - mse: 51.7274 - val_loss: 19.6304 - val_mse: 46.9308\n",
            "Epoch 87/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.5277 - mse: 51.6104 - val_loss: 19.6084 - val_mse: 46.8303\n",
            "Epoch 88/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.3850 - mse: 49.1522 - val_loss: 19.5814 - val_mse: 46.7692\n",
            "Epoch 89/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.3059 - mse: 52.4638 - val_loss: 19.6531 - val_mse: 47.4607\n",
            "Epoch 90/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.2683 - mse: 52.2842 - val_loss: 19.9304 - val_mse: 48.9825\n",
            "Epoch 91/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.2238 - mse: 50.4989 - val_loss: 19.5167 - val_mse: 46.6971\n",
            "Epoch 92/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.1126 - mse: 51.4386 - val_loss: 19.7197 - val_mse: 48.5842\n",
            "Epoch 93/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.6759 - mse: 51.1773 - val_loss: 19.5281 - val_mse: 47.4492\n",
            "Epoch 94/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.7746 - mse: 52.0113 - val_loss: 19.6104 - val_mse: 47.9474\n",
            "Epoch 95/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.5296 - mse: 50.6886 - val_loss: 19.4912 - val_mse: 47.3047\n",
            "Epoch 96/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.0516 - mse: 50.5851 - val_loss: 19.4654 - val_mse: 47.1217\n",
            "Epoch 97/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.3456 - mse: 52.4850 - val_loss: 19.4281 - val_mse: 46.5363\n",
            "Epoch 98/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.8993 - mse: 49.7497 - val_loss: 19.4817 - val_mse: 47.0778\n",
            "Epoch 99/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.1970 - mse: 51.0381 - val_loss: 19.6885 - val_mse: 48.3118\n",
            "Epoch 100/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.3242 - mse: 51.1831 - val_loss: 19.4348 - val_mse: 46.8088\n",
            "Epoch 101/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 18.7293 - mse: 50.6081 - val_loss: 19.7616 - val_mse: 48.6343\n",
            "Epoch 102/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.4892 - mse: 51.8010 - val_loss: 19.3679 - val_mse: 46.1960\n",
            "Epoch 103/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.6558 - mse: 48.4077 - val_loss: 19.5382 - val_mse: 47.8774\n",
            "Epoch 104/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.3562 - mse: 53.6939 - val_loss: 19.5773 - val_mse: 48.2009\n",
            "Epoch 105/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.7693 - mse: 49.4629 - val_loss: 19.4198 - val_mse: 47.1579\n",
            "Epoch 106/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 18.4738 - mse: 51.8720 - val_loss: 19.5387 - val_mse: 48.1136\n",
            "Epoch 107/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.1997 - mse: 50.1030 - val_loss: 19.4263 - val_mse: 47.2621\n",
            "Epoch 108/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.1123 - mse: 51.5988 - val_loss: 19.3989 - val_mse: 46.9720\n",
            "Epoch 109/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.5655 - mse: 49.3812 - val_loss: 19.2704 - val_mse: 46.1233\n",
            "Epoch 110/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.1034 - mse: 52.2443 - val_loss: 19.8274 - val_mse: 48.9340\n",
            "Epoch 111/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.7969 - mse: 49.6007 - val_loss: 19.1629 - val_mse: 44.5584\n",
            "Epoch 112/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.0832 - mse: 49.5257 - val_loss: 19.3685 - val_mse: 46.8149\n",
            "Epoch 113/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.4447 - mse: 49.8733 - val_loss: 19.4267 - val_mse: 47.2267\n",
            "Epoch 114/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 18.9386 - mse: 51.4264 - val_loss: 19.4460 - val_mse: 47.8046\n",
            "Epoch 115/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.5804 - mse: 50.9225 - val_loss: 19.1694 - val_mse: 45.5763\n",
            "Epoch 116/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 18.6252 - mse: 49.6226 - val_loss: 19.4362 - val_mse: 47.7103\n",
            "Epoch 117/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 18.8950 - mse: 50.1640 - val_loss: 19.4068 - val_mse: 47.5640\n",
            "Epoch 118/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.3021 - mse: 51.7853 - val_loss: 19.4267 - val_mse: 47.4660\n",
            "Epoch 119/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 18.4492 - mse: 50.4166 - val_loss: 19.2935 - val_mse: 46.3081\n",
            "Epoch 120/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 19.0696 - mse: 50.6129 - val_loss: 19.2739 - val_mse: 46.6379\n",
            "Epoch 121/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.5519 - mse: 48.8951 - val_loss: 19.4528 - val_mse: 48.1956\n",
            "Epoch 122/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.5715 - mse: 53.5264 - val_loss: 19.3029 - val_mse: 47.1237\n",
            "Epoch 123/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.7353 - mse: 48.3351 - val_loss: 19.1495 - val_mse: 46.1770\n",
            "Epoch 124/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.8941 - mse: 51.9640 - val_loss: 19.4536 - val_mse: 47.6301\n",
            "Epoch 125/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.8012 - mse: 50.6313 - val_loss: 19.1706 - val_mse: 46.0277\n",
            "Epoch 126/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 17.9521 - mse: 49.3992 - val_loss: 19.2879 - val_mse: 47.3183\n",
            "Epoch 127/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.9439 - mse: 53.3983 - val_loss: 19.2530 - val_mse: 46.9710\n",
            "Epoch 128/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.6596 - mse: 48.3134 - val_loss: 19.0129 - val_mse: 45.4240\n",
            "Epoch 129/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.2882 - mse: 50.2323 - val_loss: 19.3233 - val_mse: 47.9951\n",
            "Epoch 130/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.8524 - mse: 52.5819 - val_loss: 19.1745 - val_mse: 46.6187\n",
            "Epoch 131/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 18.6588 - mse: 49.5857 - val_loss: 19.2961 - val_mse: 47.5884\n",
            "Epoch 132/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.6138 - mse: 50.6828 - val_loss: 19.0598 - val_mse: 46.2276\n",
            "Epoch 133/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.5970 - mse: 51.0067 - val_loss: 19.2583 - val_mse: 47.9702\n",
            "Epoch 134/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.3664 - mse: 50.9960 - val_loss: 19.2900 - val_mse: 48.2784\n",
            "Epoch 135/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 18.5785 - mse: 50.7905 - val_loss: 18.9588 - val_mse: 46.0096\n",
            "Epoch 136/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.1428 - mse: 50.7196 - val_loss: 19.7511 - val_mse: 50.6001\n",
            "Epoch 137/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.9273 - mse: 51.0074 - val_loss: 18.8631 - val_mse: 44.4442\n",
            "Epoch 138/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.4955 - mse: 53.9251 - val_loss: 19.2955 - val_mse: 48.1463\n",
            "Epoch 139/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.1572 - mse: 47.5704 - val_loss: 18.8166 - val_mse: 44.2535\n",
            "Epoch 140/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.3774 - mse: 49.7918 - val_loss: 19.9424 - val_mse: 52.0327\n",
            "Epoch 141/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.7547 - mse: 54.1774 - val_loss: 18.9844 - val_mse: 46.3507\n",
            "Epoch 142/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 18.6094 - mse: 50.2346 - val_loss: 19.0908 - val_mse: 47.2083\n",
            "Epoch 143/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 17.6378 - mse: 49.2530 - val_loss: 19.1997 - val_mse: 47.8080\n",
            "Epoch 144/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.7773 - mse: 53.2731 - val_loss: 18.9589 - val_mse: 46.1004\n",
            "Epoch 145/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.5384 - mse: 49.3394 - val_loss: 19.1107 - val_mse: 47.1730\n",
            "Epoch 146/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.5495 - mse: 51.2560 - val_loss: 18.8653 - val_mse: 45.4063\n",
            "Epoch 147/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.3298 - mse: 50.0202 - val_loss: 19.0696 - val_mse: 47.1173\n",
            "Epoch 148/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.4473 - mse: 49.5657 - val_loss: 19.0426 - val_mse: 47.2850\n",
            "Epoch 149/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.5593 - mse: 50.7813 - val_loss: 19.2152 - val_mse: 48.6135\n",
            "Epoch 150/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 17.9200 - mse: 51.4865 - val_loss: 19.1888 - val_mse: 48.5794\n",
            "Epoch 151/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.6141 - mse: 52.2698 - val_loss: 18.7944 - val_mse: 45.9081\n",
            "Epoch 152/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.7020 - mse: 47.7258 - val_loss: 19.0972 - val_mse: 47.8782\n",
            "Epoch 153/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 18.5024 - mse: 53.5144 - val_loss: 19.1579 - val_mse: 48.2284\n",
            "Epoch 154/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.1446 - mse: 51.3812 - val_loss: 19.1588 - val_mse: 48.2275\n",
            "Epoch 155/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 19.1241 - mse: 48.5671 - val_loss: 18.9404 - val_mse: 46.5141\n",
            "Epoch 156/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.6453 - mse: 53.2691 - val_loss: 19.1953 - val_mse: 48.1839\n",
            "Epoch 157/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 18.1859 - mse: 49.2703 - val_loss: 18.8148 - val_mse: 45.7692\n",
            "Epoch 158/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.1767 - mse: 48.3677 - val_loss: 19.2342 - val_mse: 48.9995\n",
            "Epoch 159/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.4912 - mse: 54.9533 - val_loss: 18.8188 - val_mse: 46.0803\n",
            "Epoch 160/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.7236 - mse: 48.7490 - val_loss: 19.0428 - val_mse: 47.9224\n",
            "Epoch 161/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.7309 - mse: 53.6231 - val_loss: 19.1766 - val_mse: 48.8305\n",
            "Epoch 162/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 18.7820 - mse: 49.2342 - val_loss: 18.8558 - val_mse: 46.4099\n",
            "Epoch 163/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 18.5894 - mse: 51.9715 - val_loss: 19.0340 - val_mse: 47.5829\n",
            "Epoch 164/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 17.9688 - mse: 49.9826 - val_loss: 18.7392 - val_mse: 45.6019\n",
            "Epoch 165/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.0625 - mse: 48.2004 - val_loss: 19.2713 - val_mse: 48.7390\n",
            "Epoch 166/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.4075 - mse: 53.8457 - val_loss: 18.8445 - val_mse: 46.3195\n",
            "Epoch 167/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 18.0754 - mse: 48.0669 - val_loss: 18.8233 - val_mse: 46.5853\n",
            "Epoch 168/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.3003 - mse: 51.8407 - val_loss: 19.1566 - val_mse: 48.9367\n",
            "Epoch 169/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 17.8687 - mse: 50.1670 - val_loss: 18.7771 - val_mse: 46.5926\n",
            "Epoch 170/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 18.4250 - mse: 54.2458 - val_loss: 19.1986 - val_mse: 49.4639\n",
            "Epoch 171/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 18.2109 - mse: 48.6474 - val_loss: 18.4856 - val_mse: 44.4426\n",
            "Epoch 172/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.3873 - mse: 52.3253 - val_loss: 19.0491 - val_mse: 48.6479\n",
            "Epoch 173/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 17.6680 - mse: 49.1818 - val_loss: 18.5091 - val_mse: 45.2212\n",
            "Epoch 174/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.3826 - mse: 50.8337 - val_loss: 19.1112 - val_mse: 49.0799\n",
            "Epoch 175/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 18.0175 - mse: 51.5946 - val_loss: 18.8542 - val_mse: 47.3248\n",
            "Epoch 176/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.1904 - mse: 49.5644 - val_loss: 19.0145 - val_mse: 48.4802\n",
            "Epoch 177/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 18.4984 - mse: 51.9597 - val_loss: 18.7128 - val_mse: 46.5585\n",
            "Epoch 178/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 18.2431 - mse: 50.1869 - val_loss: 19.0099 - val_mse: 49.1466\n",
            "Epoch 179/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 17.7646 - mse: 53.0992 - val_loss: 18.7889 - val_mse: 47.7799\n",
            "Epoch 180/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.2760 - mse: 49.6801 - val_loss: 18.8856 - val_mse: 47.9885\n",
            "Epoch 181/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 17.8615 - mse: 50.5100 - val_loss: 19.0891 - val_mse: 49.4760\n",
            "Epoch 182/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 17.8791 - mse: 53.1427 - val_loss: 19.0202 - val_mse: 48.9965\n",
            "Epoch 183/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 17.9614 - mse: 50.4189 - val_loss: 18.5783 - val_mse: 46.0127\n",
            "Epoch 184/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.0993 - mse: 49.2698 - val_loss: 18.7888 - val_mse: 47.4061\n",
            "Epoch 185/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.2529 - mse: 50.7605 - val_loss: 18.8424 - val_mse: 47.6629\n",
            "Epoch 186/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.5597 - mse: 52.5494 - val_loss: 18.7496 - val_mse: 47.0841\n",
            "Epoch 187/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 17.8197 - mse: 49.1783 - val_loss: 18.7559 - val_mse: 47.3228\n",
            "Epoch 188/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 17.6959 - mse: 51.5884 - val_loss: 18.9466 - val_mse: 48.5818\n",
            "Epoch 189/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 17.5626 - mse: 51.0499 - val_loss: 18.6892 - val_mse: 47.0737\n",
            "Epoch 190/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 17.8140 - mse: 49.8157 - val_loss: 18.8124 - val_mse: 47.8064\n",
            "Epoch 191/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 18.1414 - mse: 52.5256 - val_loss: 18.6464 - val_mse: 46.9952\n",
            "Epoch 192/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 17.9032 - mse: 51.5066 - val_loss: 18.9424 - val_mse: 49.1747\n",
            "Epoch 193/200\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 17.6888 - mse: 50.2699 - val_loss: 18.6310 - val_mse: 46.8711\n",
            "Epoch 194/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 17.8576 - mse: 50.7980 - val_loss: 18.8668 - val_mse: 48.4541\n",
            "Epoch 195/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 17.5546 - mse: 48.8856 - val_loss: 18.7445 - val_mse: 47.8270\n",
            "Epoch 196/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 18.3763 - mse: 53.0382 - val_loss: 18.6222 - val_mse: 46.7640\n",
            "Epoch 197/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 17.8687 - mse: 49.5418 - val_loss: 18.7491 - val_mse: 47.5356\n",
            "Epoch 198/200\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 17.6900 - mse: 50.4790 - val_loss: 18.9617 - val_mse: 49.0914\n",
            "Epoch 199/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 17.2672 - mse: 52.2902 - val_loss: 18.6383 - val_mse: 47.0821\n",
            "Epoch 200/200\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 17.4420 - mse: 47.8232 - val_loss: 18.5134 - val_mse: 46.3188\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5403358668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20iPifJoxWEH",
        "colab_type": "code",
        "outputId": "5c2b1b9a-b63d-4ddc-a416-c2d6c464e728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "loss, mse = model.evaluate(test_ds)\n",
        "print(\"Mean Squared Error - Test Data\", mse)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 18.7094 - mse: 47.9443\n",
            "Mean Squared Error - Test Data 47.944283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCue-VUyxs8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}